{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3db927ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Job Title, Job Location, Company Name, Experience Required]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.naukri.com/data-analyst-jobs-in-bangalore?k=data%20analyst&l=bangalore\"\n",
    "\n",
    "html = requests.get(url).content\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "exp_required = []\n",
    "\n",
    "jobs = soup.find_all('a', class_='title ellipsis')\n",
    "\n",
    "for job in jobs:\n",
    "    title = job.find('a', class_='title ellipsis')\n",
    "    job_titles.append(title.text.strip())\n",
    "\n",
    "    location = job.find('i', class_='fleft placeholder-icons naukicon naukicon-srp-location')\n",
    "    job_locations.append(location.text.strip())\n",
    "\n",
    "    company = job.find('div', class_='companyInfo subheading')\n",
    "    company_names.append(company.text.strip())\n",
    "\n",
    "    experience = job.find('i', class_='fleft placeholder-icons naukicon naukicon-srp-experience')\n",
    "    exp_required.append(experience.text.strip())\n",
    "\n",
    "df = pd.DataFrame({'Job Title': job_titles, 'Job Location': job_locations, 'Company Name': company_names, 'Experience Required': exp_required})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401b38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Set the URL and the headers\n",
    "url = 'https://www.naukri.com/data-scientist-jobs-in-bangalore?k=data%20scientist&l=bangalore'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the job listing container\n",
    "job_container = soup.find_all('div', class_='jobTuple')\n",
    "\n",
    "# Create empty lists to store the job information\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "\n",
    "# Loop through the job listings and extract the job title, job location, and company name of each job\n",
    "for job in job_container[:10]:\n",
    "    job_title = job.find('a', class_='title').text.strip()\n",
    "    job_location = job.find('li', class_='fleft grey-text br2 placeHolderLi location').text.strip()\n",
    "    company_name = job.find('a', class_='subTitle').text.strip()\n",
    "\n",
    "    # Append the job information to the respective lists\n",
    "    job_titles.append(job_title)\n",
    "    job_locations.append(job_location)\n",
    "    company_names.append(company_name)\n",
    "\n",
    "# Create a data frame using the Pandas library\n",
    "df = pd.DataFrame({\n",
    "    'Job Title': job_titles,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7522a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.naukri.com/data-scientist-jobs-in-delhi-ncr-3-6-lakhs\"\n",
    "page = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "job_title = []\n",
    "job_location = []\n",
    "company_name = []\n",
    "exp_req = []\n",
    "\n",
    "for i in soup.find_all('a', class_=\"title fw500 ellipsis\"):\n",
    "    job_title.append(i.text.strip())\n",
    "\n",
    "for i in soup.find_all('li', class_=\"fleft grey-text br2 placeHolderLi location\"):\n",
    "    job_location.append(i.text.strip())\n",
    "\n",
    "for i in soup.find_all('a', class_=\"subTitle ellipsis fleft\"):\n",
    "    company_name.append(i.text.strip())\n",
    "\n",
    "for i in soup.find_all('li', class_=\"fleft grey-text br2 placeHolderLi experience\"):\n",
    "    exp_req.append(i.text.strip())\n",
    "\n",
    "data = {'Job Title': job_title[:10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60746557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Brand, Product Description, Price, Discount]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "\n",
    "# Send GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the first 100 sunglasses listings on the page\n",
    "sunglasses = soup.find_all('div', {'class': '_2kHMtA'})\n",
    "\n",
    "# Create empty lists to store data\n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "discounts = []\n",
    "\n",
    "# Extract the required data from each sunglasses listing\n",
    "for sunglass in sunglasses[:100]:\n",
    "    brand = sunglass.find('div', {'class': '_2WkVRV'}).text\n",
    "    description = sunglass.find('a', {'class': 'IRpwTa'}).text\n",
    "    price = sunglass.find('div', {'class': '_30jeq3 _1_WHN1'}).text\n",
    "    discount = sunglass.find('div', {'class': '_3Ay6Sb _31Dcoz pZkvcx'}).text if sunglass.find('div', {'class': '_3Ay6Sb _31Dcoz pZkvcx'}) else 'N/A'\n",
    "    \n",
    "    brands.append(brand)\n",
    "    descriptions.append(description)\n",
    "    prices.append(price)\n",
    "    discounts.append(discount)\n",
    "\n",
    "# Create a Pandas dataframe with the extracted data\n",
    "df = pd.DataFrame({'Brand': brands, 'Product Description': descriptions, 'Price': prices, 'Discount': discounts})\n",
    "\n",
    "# Print the dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaad96fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Rating, Summary, Full Review]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART'\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    \n",
    "reviews = []\n",
    "for i in range(1, 11):\n",
    "    new_url = url + '&page=' + str(i)\n",
    "    response = requests.get(new_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    all_reviews = soup.find_all('div', {'class': '_2s4DIt _1CDdy2\"'})\n",
    "\n",
    "    for review in all_reviews:\n",
    "        rating = review.find('div', {'class': '_2s4DIt _1CDdy2\"'}).text\n",
    "        summary = review.find('p', {'class': '_2-N8zT'}).text\n",
    "        full_review = review.find('div', {'class': 't-ZTKy'}).div.text\n",
    "        reviews.append([rating, summary, full_review])\n",
    "\n",
    "        if len(reviews) == 100:\n",
    "            break\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(reviews, columns=['Rating', 'Summary', 'Full Review'])\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d90ad09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Brand, Product Description, Price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request\n",
    "url = 'https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all products\n",
    "products = soup.find_all('div', {'class': '_2kHMtA'})\n",
    "\n",
    "# Create empty lists to store data\n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "\n",
    "# Scrape data for each product\n",
    "for product in products[:100]:\n",
    "    # Extract brand name\n",
    "    brand = product.find('div', {'class': '_2WkVRV'}).text\n",
    "    brands.append(brand)\n",
    "    \n",
    "    # Extract product description\n",
    "    description = product.find('a', {'class': '_2mylT6'}).text\n",
    "    descriptions.append(description)\n",
    "    \n",
    "    # Extract price\n",
    "    price = product.find('div', {'class': '_30jeq3 _1_WHN1'}).text\n",
    "    prices.append(price)\n",
    "\n",
    "# Create a pandas dataframe\n",
    "sneakers_df = pd.DataFrame({'Brand': brands, 'Product Description': descriptions, 'Price': prices})\n",
    "\n",
    "# Print dataframe\n",
    "print(sneakers_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90229a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Set the headers and URL\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "url = \"https://www.amazon.in/s?k=laptop&rh=n%3A1375424031&ref=nb_sb_noss\"\n",
    "\n",
    "# Send GET request to the URL\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the search bar and enter \"Intel Core i7\" and click on the search button\n",
    "search_bar = soup.find('input', {'id': 'twotabsearchtextbox'})\n",
    "search_bar.clear()\n",
    "search_bar.send_keys(\"Intel Core i7\")\n",
    "search_button = soup.find('input', {'id': 'nav-search-submit-button'})\n",
    "search_button.click()\n",
    "\n",
    "# Scrape data for the first 10 laptops\n",
    "laptops = []\n",
    "results = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "for result in results[:10]:\n",
    "    title = result.find('h2').text.strip()\n",
    "    rating = result.find('span', {'class': 'a-icon-alt'}).text.split()[0]\n",
    "    price = result.find('span', {'class': 'a-price-whole'}).text.replace(',', '')\n",
    "    laptops.append({'Title': title, 'Ratings': rating, 'Price': price})\n",
    "\n",
    "# Create a pandas dataframe and display the data\n",
    "df = pd.DataFrame(laptops)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e2539f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send an HTTP request to the URL of the webpage to be scraped\n",
    "url = \"https://www.azquotes.com/top_quotes.html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the div element containing the top quotes\n",
    "top_quotes_div = soup.find('div', {'class': \"wrap-block\"})\n",
    "\n",
    "\n",
    "# Loop through each quote and extract the quote, author, and type of quote\n",
    "for quote_div in top_quotes_div.find_all('div', {'class': 'wrap-block'}):\n",
    "    quote = quote_div.find('a', {'class': 'title'})['title']\n",
    "    author = quote_div.find('div', {'class': 'author'}).text.strip()\n",
    "    type_of_quote = quote_div.find('div', {'class': 'kw-box'}).text.strip()\n",
    "    \n",
    "    # Print the quote, author, and type of quote\n",
    "    print(f\"Quote: {quote}\")\n",
    "    print(f\"Author: {author}\")\n",
    "    print(f\"Type of Quote: {type_of_quote}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae41687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1\"\n",
    "page = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(page.content,'html.parser')\n",
    "table = soup.find('a', class_= 'table-box')\n",
    "\n",
    "rows = table.find_all('a', class_= 'table-box')\n",
    "data = []\n",
    "for row in rows:\n",
    "    cols = row.find_all('a', class_= 'table-box')\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    data.append(cols)\n",
    "\n",
    "df = pd.DataFrame(data[1:], columns=['Name', 'Born-Dead', 'Term of office', 'Remarks'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7dc827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.motor1.com/features/308149/most-expensive-new-cars-ever/'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "cars_list = soup.find_all('div', class_='slide')\n",
    "\n",
    "cars_data = []\n",
    "\n",
    "for car in cars_list:\n",
    "    name = car.find('h2').text.strip()\n",
    "    price = car.find('p', class_='price').text.strip()\n",
    "    cars_data.append({'Car Name': name, 'Price': price})\n",
    "\n",
    "df = pd.DataFrame(cars_data)\n",
    "\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
