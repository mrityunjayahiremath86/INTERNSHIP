{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2955c9bb",
   "metadata": {},
   "source": [
    "ANSWER 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd3d6da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product to search on Amazon: BOOK\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon_products(product):\n",
    "    base_url = \"https://www.amazon.in\"\n",
    "    search_url = f\"{base_url}/s?k={product}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    product_list = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
    "\n",
    "    for product in product_list:\n",
    "        title = product.find(\"span\", {\"class\": \"a-size-medium a-color-base a-text-normal\"})\n",
    "        price = product.find(\"span\", {\"class\": \"a-offscreen\"})\n",
    "        if title and price:\n",
    "            print(\"Title:\", title.text)\n",
    "            print(\"Price:\", price.text)\n",
    "            print()\n",
    "\n",
    "search_term = input(\"Enter the product to search on Amazon: \")\n",
    "search_amazon_products(search_term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb64ec",
   "metadata": {},
   "source": [
    "ANSWER 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0d06e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "def scrape_product_details(product):\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    service = Service('C:\\WebDriver\\chromedriver.exe')  \n",
    "\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    base_url = \"https://www.amazon.in\"\n",
    "    search_url = f\"{base_url}/s?k={product}\"\n",
    "\n",
    "    driver.get(search_url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    product_data = []\n",
    "    page_number = 1\n",
    "\n",
    "    while True:\n",
    "        product_list = driver.find_elements(By.CSS_SELECTOR, '[data-component-type=\"s-search-result\"]')\n",
    "\n",
    "        for product in product_list:\n",
    "            details = {}\n",
    "\n",
    "            try:\n",
    "                title_element = product.find_element(By.CSS_SELECTOR, '.a-size-medium.a-color-base.a-text-normal')\n",
    "                title = title_element.text.strip()\n",
    "            except:\n",
    "                title = \"-\"\n",
    "\n",
    "            try:\n",
    "                price_element = product.find_element(By.CSS_SELECTOR, '.a-offscreen')\n",
    "                price = price_element.text.strip()\n",
    "            except:\n",
    "                price = \"-\"\n",
    "\n",
    "            try:\n",
    "                return_exchange_element = product.find_element(By.CSS_SELECTOR, '.a-row.a-size-small')\n",
    "                return_exchange = return_exchange_element.text.strip()\n",
    "            except:\n",
    "                return_exchange = \"-\"\n",
    "\n",
    "            try:\n",
    "                delivery_element = product.find_element(By.CSS_SELECTOR, '.a-row.s-align-children-center')\n",
    "                delivery = delivery_element.text.strip()\n",
    "            except:\n",
    "                delivery = \"-\"\n",
    "\n",
    "            try:\n",
    "                availability_element = product.find_element(By.CSS_SELECTOR, '.a-size-base.a-color-success.s-nowrap')\n",
    "                availability = availability_element.text.strip()\n",
    "            except:\n",
    "                availability = \"-\"\n",
    "\n",
    "            try:\n",
    "                product_link_element = product.find_element(By.CSS_SELECTOR, '.a-link-normal.s-no-outline')\n",
    "                product_link = base_url + product_link_element.get_attribute('href')\n",
    "            except:\n",
    "                product_link = \"-\"\n",
    "\n",
    "            brand = title.split()[0]\n",
    "\n",
    "            details[\"Brand\"] = brand\n",
    "            details[\"Product\"] = title\n",
    "            details[\"Price\"] = price\n",
    "            details[\"Return/Exchange\"] = return_exchange\n",
    "            details[\"Expected Delivery\"] = delivery\n",
    "            details[\"Availability\"] = availability\n",
    "            details[\"Product URL\"] = product_link\n",
    "\n",
    "            product_data.append(details)\n",
    "\n",
    "        next_page_element = driver.find_element(By.CSS_SELECTOR, '.s-pagination-item.s-pagination-next')\n",
    "        next_page_link = next_page_element.get_attribute('href')\n",
    "\n",
    "        if page_number >= 3 or not next_page_link:\n",
    "            break\n",
    "\n",
    "        action = ActionChains(driver)\n",
    "        action.move_to_element(next_page_element).perform()\n",
    "        driver.execute_script(\"arguments[0].click();\", next_page_element)\n",
    "        time.sleep(2)\n",
    "        page_number += 1\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    df = pd.DataFrame(product_data)\n",
    "    return df\n",
    "\n",
    "\n",
    "search_term = input(\"Enter the product to search on Amazon: \")\n",
    "data_frame = scrape_product_details(search_term)\n",
    "\n",
    "# Save dataframe to CSV\n",
    "data_frame.to_csv(\"amazon_products.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c336b3bb",
   "metadata": {},
   "source": [
    "ANSWER 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40324c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def scrape_images(keyword, num_images):\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    service = Service('C:\\WebDriver\\chromedriver.exe')  \n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    base_url = 'https://images.google.com/'\n",
    "    search_url = base_url + 'search?q=' + keyword\n",
    "\n",
    "    driver.get(search_url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    images = []\n",
    "    image_count = 0\n",
    "\n",
    "    while image_count < num_images:\n",
    "        thumbnail_elements = driver.find_elements(By.CSS_SELECTOR, 'img.Q4LuWd')\n",
    "\n",
    "        for thumbnail in thumbnail_elements:\n",
    "            thumbnail.click()\n",
    "            time.sleep(1)\n",
    "\n",
    "            try:\n",
    "                image_element = driver.find_element(By.CSS_SELECTOR, '.n3VNCb')\n",
    "                image_url = image_element.get_attribute('src')\n",
    "\n",
    "                if image_url and 'http' in image_url:\n",
    "                    images.append(image_url)\n",
    "                    image_count += 1\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if image_count >= num_images:\n",
    "                break\n",
    "\n",
    "        scroll_element = driver.find_element(By.XPATH, '//body')\n",
    "        driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", scroll_element)\n",
    "        time.sleep(2)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return images\n",
    "\n",
    "# Keywords and number of images to scrape\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "num_images_per_keyword = 10\n",
    "\n",
    "all_images = []\n",
    "\n",
    "for keyword in keywords:\n",
    "    images = scrape_images(keyword, num_images_per_keyword)\n",
    "    all_images.extend(images)\n",
    "\n",
    "# Download and save the images\n",
    "for i, image_url in enumerate(all_images):\n",
    "    response = requests.get(image_url)\n",
    "    with open(f'{keywords[i // num_images_per_keyword]}_{i % num_images_per_keyword + 1}.jpg', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "print(\"Image scraping completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae65940d",
   "metadata": {},
   "source": [
    "ANSWER 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def scrape_smartphone_details(product):\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    service = Service('path_to_chromedriver')  # Replace 'path_to_chromedriver' with the actual path to chromedriver executable\n",
    "\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    base_url = \"https://www.flipkart.com\"\n",
    "    search_url = f\"{base_url}/search?q={product}\"\n",
    "\n",
    "    driver.get(search_url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    smartphone_data = []\n",
    "\n",
    "    product_list = driver.find_elements(By.CSS_SELECTOR, '._2kHMtA')\n",
    "\n",
    "    for product in product_list:\n",
    "        details = {}\n",
    "\n",
    "        try:\n",
    "            brand_element = product.find_element(By.CSS_SELECTOR, '._4rR01T')\n",
    "            brand = brand_element.text.strip()\n",
    "        except:\n",
    "            brand = \"-\"\n",
    "\n",
    "        try:\n",
    "            name_element = product.find_element(By.CSS_SELECTOR, 'aIRrpU')\n",
    "            name = name_element.text.strip()\n",
    "        except:\n",
    "            name = \"-\"\n",
    "\n",
    "        try:\n",
    "            color_element = product.find_element(By.CSS_SELECTOR, '._3LWZlK')\n",
    "            color = color_element.text.strip()\n",
    "        except:\n",
    "            color = \"-\"\n",
    "\n",
    "        try:\n",
    "            ram_element = product.find_element(By.XPATH, './/ul[@class=\"_1xgFaf\"]/li[1]')\n",
    "            ram = ram_element.text.strip()\n",
    "        except:\n",
    "            ram = \"-\"\n",
    "\n",
    "        try:\n",
    "            storage_element = product.find_element(By.XPATH, './/ul[@class=\"_1xgFaf\"]/li[2]')\n",
    "            storage = storage_element.text.strip()\n",
    "        except:\n",
    "            storage = \"-\"\n",
    "\n",
    "        try:\n",
    "            primary_camera_element = product.find_element(By.XPATH, './/ul[@class=\"_1xgFaf\"]/li[3]')\n",
    "            primary_camera = primary_camera_element.text.strip()\n",
    "        except:\n",
    "            primary_camera = \"-\"\n",
    "\n",
    "        try:\n",
    "            secondary_camera_element = product.find_element(By.XPATH, './/ul[@class=\"_1xgFaf\"]/li[4]')\n",
    "            secondary_camera = secondary_camera_element.text.strip()\n",
    "        except:\n",
    "            secondary_camera = \"-\"\n",
    "\n",
    "        try:\n",
    "            display_element = product.find_element(By.XPATH, './/ul[@class=\"_1xgFaf\"]/li[5]')\n",
    "            display = display_element.text.strip()\n",
    "        except:\n",
    "            display = \"-\"\n",
    "\n",
    "        try:\n",
    "            battery_element = product.find_element(By.XPATH, './/ul[@class=\"_1xgFaf\"]/li[6]')\n",
    "            battery = battery_element.text.strip()\n",
    "        except:\n",
    "            battery = \"-\"\n",
    "\n",
    "        try:\n",
    "            price_element = product.find_element(By.CSS_SELECTOR, '._30jeq3._1_WHN1')\n",
    "            price = price_element.text.strip()\n",
    "        except:\n",
    "            price = \"-\"\n",
    "\n",
    "        try:\n",
    "            product_link_element = product.find_element(By.CSS_SELECTOR, '._1AtVbE')\n",
    "            product_link = base_url + product_link_element.get_attribute('href')\n",
    "        except:\n",
    "            product_link = \"-\"\n",
    "\n",
    "        details[\"Brand\"] = brand\n",
    "        details[\"Smartphone\"] = name\n",
    "        details[\"Colour\"] = color\n",
    "        details[\"RAM\"] = ram\n",
    "        details[\"Storage(ROM)\"] = storage\n",
    "        details[\"Primary Camera\"] = primary_camera\n",
    "        details[\"Secondary Camera\"] = secondary_camera\n",
    "        details[\"Display Size\"] = display\n",
    "        details[\"Battery Capacity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34be3d1",
   "metadata": {},
   "source": [
    "ANSWER 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a950026",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f72d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def get_coordinates(city):\n",
    "    geolocator = Nominatim(user_agent=\"my_app\")\n",
    "    location = geolocator.geocode(city)\n",
    "    \n",
    "    if location:\n",
    "        latitude = location.latitude\n",
    "        longitude = location.longitude\n",
    "        return latitude, longitude\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "city = input(\"Enter the city name: \")\n",
    "coordinates = get_coordinates(city)\n",
    "\n",
    "if coordinates:\n",
    "    latitude, longitude = coordinates\n",
    "    print(f\"Coordinates of {city}: Latitude: {latitude}, Longitude: {longitude}\")\n",
    "else:\n",
    "    print(\"Unable to find coordinates for the given city.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10dff28",
   "metadata": {},
   "source": [
    "answer 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c73ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_funding_deals():\n",
    "    url = \"https://trak.in/india-startup-funding-investment-2015/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    table = soup.find('table', {'id': 'tablepress-48'})\n",
    "\n",
    "    dates = []\n",
    "    startups = []\n",
    "    industries = []\n",
    "    sub_industries = []\n",
    "    cities = []\n",
    "    investors = []\n",
    "    investments = []\n",
    "    amounts = []\n",
    "    \n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        dates.append(columns[1].text.strip())\n",
    "        startups.append(columns[2].text.strip())\n",
    "        industries.append(columns[3].text.strip())\n",
    "        sub_industries.append(columns[4].text.strip())\n",
    "        cities.append(columns[5].text.strip())\n",
    "        investors.append(columns[6].text.strip())\n",
    "        investments.append(columns[7].text.strip())\n",
    "        amounts.append(columns[8].text.strip())\n",
    "\n",
    "    data = {\n",
    "        \"Date\": dates,\n",
    "        \"Startup\": startups,\n",
    "        \"Industry\": industries,\n",
    "        \"Sub-Industry\": sub_industries,\n",
    "        \"City\": cities,\n",
    "        \"Investor\": investors,\n",
    "        \"Investment\": investments,\n",
    "        \"Amount\": amounts\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Scrape funding deals for the second quarter\n",
    "funding_deals = scrape_funding_deals()\n",
    "\n",
    "# Filter the deals for the second quarter (January 2021 to March 2021)\n",
    "start_date = \"2021-01-01\"\n",
    "end_date = \"2021-03-31\"\n",
    "filtered_deals = funding_deals[(funding_deals[\"Date\"] >= start_date) & (funding_deals[\"Date\"] <= end_date)]\n",
    "\n",
    "# Save the results to a CSV file\n",
    "filtered_deals.to_csv(\"funding_deals_q2_2021.csv\", index=False)\n",
    "\n",
    "print(\"Funding deals for the second quarter have been scraped and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea41a8",
   "metadata": {},
   "source": [
    "answer7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c5a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    laptops = soup.find_all('div', {'class': 'TopNumbeHeading'})\n",
    "\n",
    "    laptop_details = []\n",
    "\n",
    "    for laptop in laptops:\n",
    "        details = {}\n",
    "        details['Name'] = laptop.text.strip()\n",
    "\n",
    "        laptop_spec = laptop.find_next('div', {'class': 'Specs-Wrap'})\n",
    "        specs = laptop_spec.find_all('div', {'class': 'Specs'})\n",
    "        \n",
    "        for spec in specs:\n",
    "            spec_name = spec.find('div', {'class': 'heading'}).text.strip()\n",
    "            spec_value = spec.find('div', {'class': 'value'}).text.strip()\n",
    "            details[spec_name] = spec_value\n",
    "        \n",
    "        laptop_details.append(details)\n",
    "\n",
    "    return laptop_details\n",
    "\n",
    "# Scrape details of the best gaming laptops\n",
    "gaming_laptops = scrape_gaming_laptops()\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(gaming_laptops)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df.to_csv(\"best_gaming_laptops.csv\", index=False)\n",
    "\n",
    "print(\"Details of the best gaming laptops have been scraped and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ab3eb",
   "metadata": {},
   "source": [
    "ANSWER  8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f781ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    billionaires = soup.find_all('div', {'class': 'personList'})\n",
    "\n",
    "    billionaire_details = []\n",
    "\n",
    "    for billionaire in billionaires:\n",
    "        details = {}\n",
    "        details['Rank'] = billionaire.find('div', {'class': 'rank'}).text.strip()\n",
    "        details['Name'] = billionaire.find('div', {'class': 'name'}).text.strip()\n",
    "        details['Net worth'] = billionaire.find('div', {'class': 'netWorth'}).text.strip()\n",
    "        details['Age'] = billionaire.find('div', {'class': 'age'}).text.strip()\n",
    "        details['Citizenship'] = billionaire.find('div', {'class': 'countryOfCitizenship'}).text.strip()\n",
    "        details['Source'] = billionaire.find('div', {'class': 'source'}).text.strip()\n",
    "        details['Industry'] = billionaire.find('div', {'class': 'category'}).text.strip()\n",
    "        \n",
    "        billionaire_details.append(details)\n",
    "\n",
    "    return billionaire_details\n",
    "\n",
    "# Scrape details of all billionaires\n",
    "billionaires = scrape_billionaires()\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(billionaires)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df.to_csv(\"billionaires_details.csv\", index=False)\n",
    "\n",
    "print(\"Details of all billionaires have been scraped and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde92d0f",
   "metadata": {},
   "source": [
    "ANSWER 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc06776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88819278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "def get_video_comments(https://www.youtube.com/watch?v=HwVh8pmOot4, AIzaSyC8f_E4SRkKyJzZjLLeJYEWazc7espwDpM, max_results=500):\n",
    "    # Build the YouTube Data API client\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Retrieve the comments using the video ID\n",
    "    comments = []\n",
    "    nextPageToken = None\n",
    "\n",
    "    while len(comments) < max_results:\n",
    "        # Fetch a page of comments\n",
    "        response = youtube.commentThreads().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id,\n",
    "            maxResults=min(100, max_results - len(comments)),\n",
    "            pageToken=nextPageToken\n",
    "        ).execute()\n",
    "\n",
    "        # Extract comment details\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_details = {\n",
    "                'Comment': comment['textDisplay'],\n",
    "                'Upvotes': comment['likeCount'],\n",
    "                'Time': comment['publishedAt']\n",
    "            }\n",
    "            comments.append(comment_details)\n",
    "\n",
    "        # Check if there are more comments to retrieve\n",
    "        if 'nextPageToken' in response:\n",
    "            nextPageToken = response['nextPageToken']\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "# Set the video ID and API key\n",
    "video_id = 'https://www.youtube.com/watch?v=HwVh8pmOot4'\n",
    "api_key = 'AIzaSyC8f_E4SRkKyJzZjLLeJYEWazc7espwDpM'\n",
    "\n",
    "# Get the comments from the YouTube video\n",
    "comments = get_video_comments(https://www.youtube.com/watch?v=HwVh8pmOot4, AIzaSyC8f_E4SRkKyJzZjLLeJYEWazc7espwDpM)\n",
    "\n",
    "# Print the comments\n",
    "for comment in comments:\n",
    "    print(f\"Comment: {comment['Comment']}\")\n",
    "    print(f\"Upvotes: {comment['Upvotes']}\")\n",
    "    print(f\"Time: {comment['Time']}\")\n",
    "    print()\n",
    "\n",
    "# Save the comments to a file, such as CSV or JSON, if desired\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ff394d",
   "metadata": {},
   "source": [
    "ANSWER 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf61027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_hostels():\n",
    "    url = \"https://www.hostelworld.com/search?search_keywords=London,%20England&country=England&city=London&date_from=2022-12-01&date_to=2022-12-03&number_of_guests=1&page=1\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    hostels = soup.find_all('div', {'class': 'hostel-container'})\n",
    "\n",
    "    hostel_details = []\n",
    "\n",
    "    for hostel in hostels:\n",
    "        details = {}\n",
    "        details['Hostel Name'] = hostel.find('h2', {'class': 'title'}).text.strip()\n",
    "        details['Distance from City Centre'] = hostel.find('span', {'class': 'distance'}).text.strip()\n",
    "        details['Ratings'] = hostel.find('div', {'class': 'score orange'}).text.strip()\n",
    "        details['Total Reviews'] = hostel.find('div', {'class': 'reviews'}).text.strip()\n",
    "        details['Overall Reviews'] = hostel.find('div', {'class': 'rating-factors'}).text.strip()\n",
    "        details['Privates from Price'] = hostel.find('div', {'class': 'price'}).text.strip()\n",
    "        details['Dorms from Price'] = hostel.find('span', {'class': 'price'}).text.strip()\n",
    "        details['Facilities'] = [facility.text.strip() for facility in hostel.find_all('span', {'class': 'facilities-label'})]\n",
    "        details['Property Description'] = hostel.find('div', {'class': 'description'}).text.strip()\n",
    "        \n",
    "        hostel_details.append(details)\n",
    "\n",
    "    return hostel_details\n",
    "\n",
    "# Scrape details of all available hostels in London\n",
    "hostels = scrape_hostels()\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(hostels)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df.to_csv(\"london_hostels.csv\", index=False)\n",
    "\n",
    "print(\"Hostel details in London have been scraped and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0024e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
